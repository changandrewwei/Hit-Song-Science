{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1705f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ac16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6900e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download([     \"names\",\n",
    "     \"stopwords\",\n",
    "     \"state_union\",\n",
    "     \"twitter_samples\",\n",
    "     \"movie_reviews\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93f6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833ff99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aadab26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 120,\n",
      " '%': 10,\n",
      " \"'\": 10,\n",
      " \"''\": 10,\n",
      " \"'M\": 10,\n",
      " \"'m\": 40,\n",
      " \"'s\": 70,\n",
      " \"'strawberries\": 10,\n",
      " \"'ve\": 20,\n",
      " '(': 20,\n",
      " ')': 20,\n",
      " ',': 450,\n",
      " '-': 20,\n",
      " '.': 320,\n",
      " '..': 40,\n",
      " '...': 50,\n",
      " '.I': 10,\n",
      " '0:27': 10,\n",
      " '1': 10,\n",
      " '10': 10,\n",
      " '10:26': 10,\n",
      " '12': 10,\n",
      " '15': 10,\n",
      " '1776': 10,\n",
      " '1:02': 10,\n",
      " '2': 10,\n",
      " '2016': 10,\n",
      " '2018': 10,\n",
      " '2024': 180,\n",
      " '2024‚ù§': 10,\n",
      " '2024üéâ': 10,\n",
      " '2024üòÆ': 10,\n",
      " '24:14': 10,\n",
      " '2:9': 10,\n",
      " '3': 20,\n",
      " '31': 10,\n",
      " '3:16': 10,\n",
      " '3:19': 10,\n",
      " '3:36': 10,\n",
      " '3:5': 10,\n",
      " '5:8': 10,\n",
      " '6': 10,\n",
      " '6:4-6': 10,\n",
      " '6:7': 10,\n",
      " '7:13-14': 10,\n",
      " '7:38': 10,\n",
      " '8': 50,\n",
      " '8:14': 10,\n",
      " '8ÎÖÑÏ†ÑÏù¥ÎùºÍ≥†Ïöî': 10,\n",
      " '9:27': 10,\n",
      " ':': 20,\n",
      " '<': 20,\n",
      " '?': 180,\n",
      " 'AGAINNNN': 10,\n",
      " 'ARE': 10,\n",
      " 'Acts': 10,\n",
      " 'Allie': 10,\n",
      " 'And': 40,\n",
      " 'Anyone': 40,\n",
      " 'Are': 10,\n",
      " 'Australian': 10,\n",
      " 'Bieber': 30,\n",
      " 'Bro': 10,\n",
      " 'But': 10,\n",
      " 'Can': 20,\n",
      " 'Christ': 10,\n",
      " 'Comfort': 10,\n",
      " 'Company': 10,\n",
      " 'Cookie': 10,\n",
      " 'Corinthians': 10,\n",
      " 'Covid': 10,\n",
      " 'DAY': 10,\n",
      " 'Do': 10,\n",
      " 'EVERY': 10,\n",
      " 'Either': 10,\n",
      " 'Enter': 10,\n",
      " 'FOUND': 10,\n",
      " 'FREAKING': 10,\n",
      " 'Feb': 10,\n",
      " 'For': 50,\n",
      " 'From': 10,\n",
      " 'Fuxk': 10,\n",
      " 'Galatians': 10,\n",
      " 'God': 100,\n",
      " 'HERE': 10,\n",
      " 'He': 30,\n",
      " 'Hebrews': 30,\n",
      " 'Holy': 10,\n",
      " 'Hope': 10,\n",
      " 'How': 10,\n",
      " 'Hyunjin': 20,\n",
      " 'I': 240,\n",
      " 'IM': 10,\n",
      " 'IN': 10,\n",
      " 'IT': 10,\n",
      " 'It': 30,\n",
      " 'Its': 10,\n",
      " 'J': 10,\n",
      " 'Jesus': 10,\n",
      " 'John': 40,\n",
      " 'Just': 20,\n",
      " 'Justin': 10,\n",
      " 'LISTENED': 10,\n",
      " 'Let': 10,\n",
      " 'Love': 20,\n",
      " 'MBBS': 10,\n",
      " 'Matthew': 20,\n",
      " 'Memories': 10,\n",
      " 'Mendes': 30,\n",
      " 'Mendes.He': 10,\n",
      " 'My': 50,\n",
      " 'NET': 140,\n",
      " 'NOT': 10,\n",
      " 'No': 10,\n",
      " 'OBSESSED': 10,\n",
      " 'OLD': 10,\n",
      " 'OMG': 10,\n",
      " 'OMGG': 10,\n",
      " 'Okay': 10,\n",
      " 'Or': 10,\n",
      " 'Origins': 10,\n",
      " 'P': 10,\n",
      " 'PS': 10,\n",
      " 'Parties': 10,\n",
      " 'People': 20,\n",
      " 'Perfect': 10,\n",
      " 'Published': 10,\n",
      " 'Romans': 20,\n",
      " 'Saudades': 10,\n",
      " 'Shawn': 20,\n",
      " 'So': 10,\n",
      " 'Son': 40,\n",
      " 'Song': 20,\n",
      " 'Spirit': 20,\n",
      " 'Still': 10,\n",
      " 'Summer': 10,\n",
      " 'THIS': 10,\n",
      " 'TO': 10,\n",
      " 'TODAY': 10,\n",
      " 'Talking': 10,\n",
      " 'Tasted': 10,\n",
      " 'The': 30,\n",
      " 'Therefore': 10,\n",
      " 'Things': 10,\n",
      " 'This': 40,\n",
      " 'Troye': 40,\n",
      " 'Who': 10,\n",
      " 'Why': 10,\n",
      " 'Wolverine': 10,\n",
      " 'Wow': 10,\n",
      " 'X': 10,\n",
      " 'X-Men': 10,\n",
      " 'YOU': 10,\n",
      " '``': 10,\n",
      " 'a': 210,\n",
      " 'addictive': 10,\n",
      " 'after': 20,\n",
      " 'again': 30,\n",
      " 'age': 10,\n",
      " 'ago': 10,\n",
      " 'ai': 10,\n",
      " 'all': 50,\n",
      " 'also': 20,\n",
      " 'always': 10,\n",
      " 'am': 30,\n",
      " 'amazing': 20,\n",
      " 'an': 20,\n",
      " 'and': 290,\n",
      " 'answered': 10,\n",
      " 'anymore': 10,\n",
      " 'anyone': 60,\n",
      " 'apostasy': 10,\n",
      " 'appointed': 10,\n",
      " 'are': 70,\n",
      " 'as': 30,\n",
      " 'at': 10,\n",
      " 'awakening': 10,\n",
      " 'baby': 10,\n",
      " 'back': 40,\n",
      " 'banger': 10,\n",
      " 'bblhüò¢': 10,\n",
      " 'be': 80,\n",
      " 'beautiful': 40,\n",
      " 'because': 30,\n",
      " 'become': 10,\n",
      " 'been': 40,\n",
      " 'before': 10,\n",
      " 'begging': 10,\n",
      " 'believe': 20,\n",
      " 'believes': 30,\n",
      " 'better': 10,\n",
      " 'between': 10,\n",
      " 'big': 30,\n",
      " 'born': 10,\n",
      " 'both': 10,\n",
      " 'bulldog': 10,\n",
      " 'but': 80,\n",
      " 'buttyfula': 10,\n",
      " 'by': 30,\n",
      " 'came': 30,\n",
      " 'can': 10,\n",
      " 'carefree': 10,\n",
      " 'case': 10,\n",
      " 'cause': 10,\n",
      " 'check': 10,\n",
      " 'chenle': 20,\n",
      " 'cigarette': 10,\n",
      " 'cigarettes': 10,\n",
      " 'colliding': 10,\n",
      " 'come': 20,\n",
      " 'coment√°rios': 10,\n",
      " 'coming': 10,\n",
      " 'committed': 10,\n",
      " 'comparison': 10,\n",
      " 'contempt': 10,\n",
      " 'corona': 10,\n",
      " 'could': 20,\n",
      " 'crazy': 10,\n",
      " 'created': 10,\n",
      " 'crucifying': 10,\n",
      " 'crush': 20,\n",
      " 'damm': 10,\n",
      " 'damn': 10,\n",
      " 'day': 10,\n",
      " 'days': 10,\n",
      " 'dayüòÖ': 10,\n",
      " 'de': 10,\n",
      " 'deceived': 10,\n",
      " 'definately': 10,\n",
      " 'deliberately': 10,\n",
      " 'demonstrates': 10,\n",
      " 'destruction': 10,\n",
      " 'detroit': 10,\n",
      " 'die': 10,\n",
      " 'died': 10,\n",
      " 'difficult': 10,\n",
      " 'dis': 10,\n",
      " 'do': 10,\n",
      " 'does': 10,\n",
      " 'done': 10,\n",
      " 'dont': 20,\n",
      " 'drink': 10,\n",
      " 'ear': 10,\n",
      " 'earth': 10,\n",
      " 'edm': 10,\n",
      " 'els': 10,\n",
      " 'end': 10,\n",
      " 'ended': 10,\n",
      " 'enjoy': 10,\n",
      " 'enlightened': 10,\n",
      " 'enter': 20,\n",
      " 'era': 10,\n",
      " 'eternal': 20,\n",
      " 'euphoria': 10,\n",
      " 'even': 10,\n",
      " 'everyone': 10,\n",
      " 'evolution': 10,\n",
      " 'extremely': 10,\n",
      " 'eye': 10,\n",
      " 'face': 10,\n",
      " 'fan': 20,\n",
      " 'fav': 10,\n",
      " 'favourite': 10,\n",
      " 'feel': 10,\n",
      " 'feels': 10,\n",
      " 'fell': 20,\n",
      " 'few': 20,\n",
      " 'find': 10,\n",
      " 'flow': 10,\n",
      " 'fool': 10,\n",
      " 'for': 80,\n",
      " 'forever': 10,\n",
      " 'from': 10,\n",
      " 'fun': 10,\n",
      " 'further': 10,\n",
      " 'gate': 30,\n",
      " 'gave': 10,\n",
      " 'gay': 30,\n",
      " 'getting': 10,\n",
      " 'gift': 10,\n",
      " 'girls': 10,\n",
      " 'give': 10,\n",
      " 'gives': 10,\n",
      " 'go': 10,\n",
      " 'good': 40,\n",
      " 'goodbye': 10,\n",
      " 'gorgeous.I': 10,\n",
      " 'gospel': 10,\n",
      " 'got': 30,\n",
      " 'graceful': 10,\n",
      " 'great': 20,\n",
      " 'guava': 10,\n",
      " 'guess': 20,\n",
      " 'guy': 20,\n",
      " 'haaaaay': 10,\n",
      " 'had': 30,\n",
      " 'handsome': 10,\n",
      " 'happy': 10,\n",
      " 'has': 50,\n",
      " 'hate': 10,\n",
      " 'hating': 10,\n",
      " 'have': 80,\n",
      " 'having': 10,\n",
      " 'he': 70,\n",
      " 'health': 10,\n",
      " 'hear': 10,\n",
      " 'heard': 10,\n",
      " 'heavenly': 10,\n",
      " 'her': 10,\n",
      " 'here': 70,\n",
      " 'hereü•∫\\U0001faf6üèª': 10,\n",
      " 'hes': 10,\n",
      " 'him': 70,\n",
      " 'his': 50,\n",
      " 'holding': 10,\n",
      " 'how': 10,\n",
      " 'i': 170,\n",
      " 'icon': 10,\n",
      " 'idea': 10,\n",
      " 'if': 10,\n",
      " 'ignorance': 10,\n",
      " 'im': 30,\n",
      " 'imagined': 10,\n",
      " 'impossible': 10,\n",
      " 'in': 210,\n",
      " 'indeed': 10,\n",
      " 'influenced': 10,\n",
      " 'inhabited': 10,\n",
      " 'inspired': 10,\n",
      " 'is': 240,\n",
      " 'isn': 10,\n",
      " 'issues': 10,\n",
      " 'it': 130,\n",
      " 'jisung': 10,\n",
      " 'jisungs': 10,\n",
      " 'judgment': 10,\n",
      " 'juice': 10,\n",
      " 'just': 50,\n",
      " 'keep': 10,\n",
      " 'kid': 10,\n",
      " 'kingdom': 20,\n",
      " 'knew': 10,\n",
      " 'knowledge': 10,\n",
      " 'leads': 20,\n",
      " 'led': 10,\n",
      " 'left': 10,\n",
      " 'life': 60,\n",
      " 'like': 50,\n",
      " 'liked': 10,\n",
      " 'listen': 40,\n",
      " 'listening': 20,\n",
      " 'living': 10,\n",
      " 'looks': 10,\n",
      " 'lot.But': 10,\n",
      " 'lov': 10,\n",
      " 'love': 80,\n",
      " 'loved': 10,\n",
      " 'made': 20,\n",
      " 'many': 30,\n",
      " 'married': 10,\n",
      " 'masterpiece': 10,\n",
      " 'may': 10,\n",
      " 'maybe': 10,\n",
      " 'me': 90,\n",
      " 'mendes.He': 10,\n",
      " 'mental': 10,\n",
      " 'mentally': 10,\n",
      " 'mind': 10,\n",
      " 'miracles': 10,\n",
      " 'more': 10,\n",
      " 'much': 10,\n",
      " 'music': 30,\n",
      " 'musically': 10,\n",
      " 'my': 80,\n",
      " 'myself': 10,\n",
      " 'm√∫sica': 10,\n",
      " \"n't\": 20,\n",
      " 'name': 10,\n",
      " 'narrow': 20,\n",
      " 'nations': 10,\n",
      " 'necessarily': 10,\n",
      " 'need': 10,\n",
      " 'neither': 10,\n",
      " 'never': 10,\n",
      " 'new': 10,\n",
      " 'no': 40,\n",
      " 'nostalgic': 10,\n",
      " 'not': 70,\n",
      " 'now': 30,\n",
      " 'of': 180,\n",
      " 'old': 30,\n",
      " 'old‚ù§': 10,\n",
      " 'on': 40,\n",
      " 'once': 20,\n",
      " 'one': 70,\n",
      " 'ones': 10,\n",
      " 'only': 10,\n",
      " 'opinion': 10,\n",
      " 'or': 30,\n",
      " 'os': 10,\n",
      " 'out': 30,\n",
      " 'over': 20,\n",
      " 'own': 20,\n",
      " 'pansexual': 10,\n",
      " 'partakers': 10,\n",
      " 'passing': 10,\n",
      " 'people': 10,\n",
      " 'perish': 10,\n",
      " 'person': 20,\n",
      " 'personality': 10,\n",
      " 'played': 10,\n",
      " 'please': 10,\n",
      " 'plus': 10,\n",
      " 'preached': 10,\n",
      " 'preety': 10,\n",
      " 'prepared': 10,\n",
      " 'promise': 10,\n",
      " 'quando': 10,\n",
      " 'realised': 10,\n",
      " 'reap': 10,\n",
      " 'receiving': 10,\n",
      " 'rejects': 10,\n",
      " 'release': 10,\n",
      " 'remains': 10,\n",
      " 'remember': 30,\n",
      " 'reminds': 20,\n",
      " 'renew': 10,\n",
      " 'repent': 10,\n",
      " 'repentance': 10,\n",
      " 'replayed': 10,\n",
      " 'right': 10,\n",
      " 'rivers': 10,\n",
      " 'romantic': 10,\n",
      " 'rude': 10,\n",
      " 's': 40,\n",
      " 'sacrifice': 10,\n",
      " 'said': 20,\n",
      " 'same': 10,\n",
      " 'say': 10,\n",
      " 'says': 10,\n",
      " 'scenario': 10,\n",
      " 'scripture': 10,\n",
      " 'see': 20,\n",
      " 'seen': 10,\n",
      " 'send': 10,\n",
      " 'shit': 20,\n",
      " 'since': 20,\n",
      " 'sing': 20,\n",
      " 'singer': 10,\n",
      " 'sinners': 10,\n",
      " 'sinning': 10,\n",
      " 'sins': 20,\n",
      " 'sivan': 10,\n",
      " 'sivans': 10,\n",
      " 'so': 120,\n",
      " 'sobre': 10,\n",
      " 'solemn': 10,\n",
      " 'some': 10,\n",
      " 'song': 100,\n",
      " 'song.Therse': 10,\n",
      " 'songs': 20,\n",
      " 'songwritter': 10,\n",
      " 'sons': 10,\n",
      " 'sound': 10,\n",
      " 'sounded': 10,\n",
      " 'sows': 10,\n",
      " 'spacious': 10,\n",
      " 'spirit': 10,\n",
      " 'stayed': 10,\n",
      " 'still': 50,\n",
      " 'structured': 10,\n",
      " 'students': 10,\n",
      " 'success': 10,\n",
      " 'such': 10,\n",
      " 'suggest': 10,\n",
      " 'summer': 10,\n",
      " 't': 40,\n",
      " 'talented': 20,\n",
      " 'tasted': 10,\n",
      " 'tell': 10,\n",
      " 'testimony': 10,\n",
      " 'that': 110,\n",
      " 'the': 380,\n",
      " 'them': 10,\n",
      " 'themselves': 10,\n",
      " 'then': 40,\n",
      " 'there': 30,\n",
      " 'they': 10,\n",
      " 'things': 10,\n",
      " 'think': 10,\n",
      " 'this': 230,\n",
      " 'those': 20,\n",
      " 'through': 20,\n",
      " 'throughout': 10,\n",
      " 'tiktok': 10,\n",
      " 'time': 20,\n",
      " 'times': 10,\n",
      " 'to': 290,\n",
      " 'together': 10,\n",
      " 'ton': 10,\n",
      " 'too.He': 20,\n",
      " 'totally': 20,\n",
      " 'touch': 10,\n",
      " 'troye': 50,\n",
      " 'troyes': 10,\n",
      " 'truth': 20,\n",
      " 'turn': 20,\n",
      " 'twink': 10,\n",
      " 'two': 10,\n",
      " 'u': 20,\n",
      " 'unless': 10,\n",
      " 'up': 20,\n",
      " 'us': 30,\n",
      " 'use': 10,\n",
      " 'vibes': 20,\n",
      " 'video': 20,\n",
      " 'videos': 10,\n",
      " 'vocals': 10,\n",
      " 'voice': 10,\n",
      " 'wahyu': 10,\n",
      " 'want': 10,\n",
      " 'wanted': 10,\n",
      " 'wanting': 10,\n",
      " 'was': 60,\n",
      " 'wasn': 10,\n",
      " 'watch': 10,\n",
      " 'watching': 10,\n",
      " 'water': 10,\n",
      " 'water.': 10,\n",
      " 'way': 60,\n",
      " 'ways': 10,\n",
      " 'we': 40,\n",
      " 'were': 10,\n",
      " 'what': 30,\n",
      " 'when': 20,\n",
      " 'which': 10,\n",
      " 'while': 10,\n",
      " 'who': 130,\n",
      " 'whole': 10,\n",
      " 'wide': 10,\n",
      " 'will': 80,\n",
      " 'wiped': 10,\n",
      " 'with': 10,\n",
      " 'within': 10,\n",
      " 'wolverine': 10,\n",
      " 'woman': 10,\n",
      " 'word': 20,\n",
      " 'world': 10,\n",
      " 'worlds': 10,\n",
      " 'would': 10,\n",
      " 'wrath': 10,\n",
      " 'wtf': 10,\n",
      " 'years': 60,\n",
      " 'yearsüò≠can': 10,\n",
      " 'you': 30,\n",
      " 'young': 10,\n",
      " 'your': 20,\n",
      " 'yoursüßë\\u200d‚öïÔ∏è': 10,\n",
      " 'youth': 60,\n",
      " 'yrs': 10,\n",
      " '‡§ï‡•É‡§∑‡•ç‡§£‡§æ': 40,\n",
      " '‡§ó‡§Ç‡§ó‡§æ': 50,\n",
      " '‡§Æ‡§π‡§æ‡§¶‡•á‡§µ': 10,\n",
      " '‡§∞‡§æ‡§ß‡§æ': 40,\n",
      " '‡§∞‡§æ‡§ß‡•á': 40,\n",
      " '‡§∞‡§æ‡§Æ': 40,\n",
      " '‡§π‡§∞': 40,\n",
      " '‡§π‡§∞‡•á': 180,\n",
      " '‚Äò': 20,\n",
      " '‚Äô': 100,\n",
      " '‚Äú': 10,\n",
      " '‚ù§': 70,\n",
      " '‚ù§‚ù§‚ù§': 30,\n",
      " '‚ù§‚ù§‚ù§‚ù§': 10,\n",
      " '‚ù§Ô∏è': 10,\n",
      " 'Í∞ÄÏÇ¨Í∞Ä': 10,\n",
      " 'Í∞êÏÑ±': 10,\n",
      " 'ÎÑàÎ¨¥': 10,\n",
      " 'ÎÑàÎ¨¥Ï¢ãÎã§\\U0001faf6': 10,\n",
      " 'Îì§Ïñ¥ÎèÑ': 10,\n",
      " 'ÎßàÏù¥': 10,\n",
      " 'ÏÑ§Î†à': 10,\n",
      " 'Ïñ∏Ï†ú': 10,\n",
      " 'ÏòàÏÅòÍ≥†': 10,\n",
      " 'Ïö∏Í≥†Ïã∂Ïñ¥Ïöî': 10,\n",
      " 'Ïú†Ïì∞Ïù¥Ï•¨Ïñ¥Ïì∞': 10,\n",
      " 'Ïù¥Í≤å': 10,\n",
      " 'Ïù¥Îïå': 10,\n",
      " 'Ï©îÏóàÏßÄ': 10,\n",
      " 'Ï≤úÏßÄÍ∞Ä': 10,\n",
      " 'Ïª§Î≤ÑÌïú': 10,\n",
      " 'üåπ': 10,\n",
      " 'üçåüçåüß™üß´‚õëÔ∏èüß¨üóûÔ∏èüèÄüëå‚ò¢Ô∏èüôè‚ù§Ô∏èüèîÔ∏è': 10,\n",
      " 'üëå': 10,\n",
      " 'üòÇ': 10,\n",
      " 'üòÇüòÇ': 10,\n",
      " 'üòÖüòÖ': 10,\n",
      " 'üòä': 10,\n",
      " 'üòäüéâ‚ù§': 10,\n",
      " 'üòù': 10,\n",
      " 'üò¢': 30,\n",
      " 'üò≠': 10,\n",
      " 'üò≠üò≠': 10,\n",
      " 'üòÆ': 10,\n",
      " 'üôÉ': 10,\n",
      " 'ü•∞': 10,\n",
      " 'ü•∞ü•∞': 10,\n",
      " 'ü•¥': 10,\n",
      " '\\U0001f979üò≠': 10,\n",
      " 'ü´Ä': 10,\n",
      " '\\U0001faf6üèΩ': 10}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ÊåáÂÆöExcel Ê™îÊ°àË∑ØÂæë\n",
    "excel_file_path = 'C:/Users/USER/songpaperdata/YOUTHTroye Sivan.xlsx'\n",
    "\n",
    "# ËÆÄÂèñ Excel Ê™îÊ°à‰∏≠ÁöÑÊñáÊú¨ÔºåÂÅáË®≠ÊñáÊú¨ÊâÄÂú®ÁöÑÊ¨Ñ‰ΩçÁÇ∫ 'ColumnName'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "text_column = df['Text'].tolist()\n",
    "\n",
    "# ‰ΩøÁî® nltk Êñ∑Ë©û\n",
    "tokenized_text = [word_tokenize(sentence) for sentence in text_column]\n",
    "\n",
    "# ÊâìÂç∞Êñ∑Ë©ûÂæåÁöÑÁµêÊûú\n",
    "# pprint(tokenized_text, width=79, compact=True)\n",
    "\n",
    "# Â∞áÊâÄÊúâÂè•Â≠êÁöÑÊñ∑Ë©ûÂêà‰ΩµÊàê‰∏ÄÂÄãÂàóË°®\n",
    "words = [word for sentence_words in tokenized_text for word in sentence_words]\n",
    "\n",
    "# ‰ΩøÁî® FreqDist Ë®àÁÆóË©ûÈ†ª\n",
    "fd = nltk.FreqDist(words)\n",
    "\n",
    "# ÊâìÂç∞Ë©ûÈ†ªÁµ±Ë®à\n",
    "pprint(fd, width=79, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b15f17b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.063, 'neu': 0.726, 'pos': 0.211, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "text=' '.join(words)\n",
    "scores = sia.polarity_scores(text)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a93121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I   !  22 \n",
      "280 200 175 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)\n",
    "\n",
    "fd.tabulate(3)\n",
    "\n",
    "lower_fd = nltk.FreqDist([w.lower() for w in fd])\n",
    "\n",
    "fd[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4fe8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 1079 matches:\n",
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n",
      "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
      " to make complete victory certain , America will never become a party to any pl\n",
      "nly in law and in justice . Here in America , we have labored long and hard to \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "text.concordance(\"america\", lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ca29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n"
     ]
    }
   ],
   "source": [
    "concordance_list = text.concordance_list(\"america\", lines=2)\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03c52d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I   !  22 \n",
      "280 200 175 \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(words)#Âà©Áî®vocabÈáçÊñ∞Ë®àÁÆóË©ûÈ†ª\n",
    "fd = text.vocab()\n",
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fc9de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('the', 'United', 'States') ('the', 'American', 'people') \n",
      "                          294                           185 \n"
     ]
    }
   ],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "finder.ngram_fd.most_common(2)\n",
    "finder.ngram_fd.tabulate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2903c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import pandas as pd\n",
    "\n",
    "# dataset = pd.read_excel(\"D:\\\\youtube_comments5.xlsx\")\n",
    "# for i in range(len(dataset)):\n",
    "#     list1=dataset['Text'][i]\n",
    "#     words: list[str] = nltk.word_tokenize(list1)\n",
    "#     fd = nltk.FreqDist(words)\n",
    "#     pprint(fd, width=79, compact=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ff0fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13facd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5022c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> True RT @Fight4UK: Hats off to @Nigel_Farage üëèüëèüëè\n",
      "\n",
      "Absolutely wonderful despite @BBC best attempts. \n",
      "\n",
      "#UKIP \n",
      "#YesNigel \n",
      "#TeamNigel http//t.co/Q0‚Ä¶\n",
      "> True @jobayeshopp @Sp1ns @storrmin571 @monticles @tonyclarkeuk @PeterDCooper @chhcalling @Helimed53 it's #tellanoldjokeday contributions reqd :-)\n",
      "> True RT @MartinMcArthur: Ed Milliband had the most diffcult questions on the nite but stood his ground &amp; answered strongly while cameron dodged ‚Ä¶\n",
      "> False Watching Nigel Farage live on BBC1\n",
      "> False @mz_chocl8bear dude I didn't get the dm :(\n",
      "> False RT @PigeonJon: ‚óªÔ∏èTory.\n",
      "\n",
      "‚óªÔ∏è Labour.\n",
      "\n",
      "‚óªÔ∏è Lib Dem.\n",
      "\n",
      "‚óªÔ∏è UKIP.\n",
      "\n",
      "‚òëÔ∏è Pigeon.\n",
      "> True RT @egsconservative: The Conservatives came out on top after the debate:\n",
      "Con 65%\n",
      "Lab 15%\n",
      "Gre 15%\n",
      "UKIP 5%\n",
      "Vote Joshua Etherington tomorrow! ‚Ä¶\n",
      "> True RT @GracieSamuels: #bbcqt Tories WILL make ¬£12 BILLION CUTS to the benefits of the working poor, child benefit, disability benefit &amp; NHS &amp; ‚Ä¶\n",
      "> False RT @petemcarthur: Why are Lab, Tory and LibDems so scared of democracy? Frightened that the people of Scotland have sussed out their corrup‚Ä¶\n",
      "> False Business likes the Tories because they let workers be exploited confirmed on  Question Time\n",
      "Vote @UKLabour @willscobie\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7e6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b157ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "025a0349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.05% correct\n"
     ]
    }
   ],
   "source": [
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "         if review_id in positive_review_ids:\n",
    "                correct += 1\n",
    "    else:\n",
    "         if review_id in negative_review_ids:\n",
    "                correct += 1\n",
    "                \n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36831c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed14a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc5b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "863f8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6830539",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff29856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 2                 pos : neg    =      7.3 : 1.0\n",
      "               wordcount = 3                 pos : neg    =      6.3 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      1.8 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.7 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.5 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6533333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "122b220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edd9f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f11a9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d0c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60743c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.60% - BernoulliNB\n",
      "66.33% - ComplementNB\n",
      "65.20% - MultinomialNB\n",
      "68.47% - KNeighborsClassifier\n",
      "63.73% - DecisionTreeClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.47% - RandomForestClassifier\n",
      "71.00% - LogisticRegression\n",
      "72.53% - MLPClassifier\n",
      "70.53% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    classifier.train(features[:train_count])\n",
    "    accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
