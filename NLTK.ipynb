{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1705f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ac16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6900e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download([     \"names\",\n",
    "     \"stopwords\",\n",
    "     \"state_union\",\n",
    "     \"twitter_samples\",\n",
    "     \"movie_reviews\",\n",
    "     \"averaged_perceptron_tagger\",\n",
    "     \"vader_lexicon\",\n",
    "     \"punkt\",\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2abfb3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360ddeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d93f6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "833ff99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in words if w.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "2aadab26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 150,\n",
      " '#': 10,\n",
      " '&': 10,\n",
      " \"'d\": 10,\n",
      " \"'m\": 90,\n",
      " \"'re\": 30,\n",
      " \"'s\": 90,\n",
      " \"'ve\": 130,\n",
      " '(': 170,\n",
      " ')': 180,\n",
      " ',': 490,\n",
      " '.': 130,\n",
      " '..': 10,\n",
      " '...': 40,\n",
      " '....': 10,\n",
      " '03.06.2024': 10,\n",
      " '03/06/2024': 10,\n",
      " '05/27': 10,\n",
      " '1': 10,\n",
      " '1OVE': 10,\n",
      " '2': 10,\n",
      " '2024': 300,\n",
      " '2024‚ù§': 20,\n",
      " '2024üíúüíúüíúINDIAN': 10,\n",
      " '2024üòÖ': 10,\n",
      " '2024üòä': 10,\n",
      " '2025': 20,\n",
      " '2028üëÄ': 10,\n",
      " '2050': 10,\n",
      " '3': 30,\n",
      " '4': 20,\n",
      " '4th': 10,\n",
      " '5181800000277239': 10,\n",
      " '8': 10,\n",
      " ':': 40,\n",
      " '?': 310,\n",
      " 'ADELE': 10,\n",
      " 'ADELE‚ù§': 10,\n",
      " 'ADELLE': 10,\n",
      " 'AM': 10,\n",
      " 'ANGELIC': 10,\n",
      " 'Absolutely': 10,\n",
      " 'Adele': 30,\n",
      " 'Algu√©m': 10,\n",
      " 'Am': 10,\n",
      " 'And': 20,\n",
      " 'Any': 10,\n",
      " 'Anymore': 10,\n",
      " 'Anyone': 150,\n",
      " 'Are': 10,\n",
      " 'At': 30,\n",
      " 'BILLION': 10,\n",
      " 'BLESSING': 10,\n",
      " 'Beautiful': 10,\n",
      " 'Because': 20,\n",
      " 'BosS‚Ä¢26–ª–æ—Ö–∏üíãüíãüòúüñêÔ∏è': 10,\n",
      " 'Brasil': 10,\n",
      " 'Brilliant': 10,\n",
      " 'But': 60,\n",
      " 'CGGGG': 10,\n",
      " 'California': 10,\n",
      " 'Card': 10,\n",
      " 'Chileüá®üá±': 10,\n",
      " 'DATE': 10,\n",
      " 'DURZOY': 10,\n",
      " 'Dad': 10,\n",
      " 'Did': 10,\n",
      " 'Easy': 10,\n",
      " 'Enfim': 10,\n",
      " 'FRICKIN': 10,\n",
      " 'ForreVE': 10,\n",
      " 'From': 10,\n",
      " 'GLOBAL': 10,\n",
      " 'Greetings': 10,\n",
      " 'HAPPY': 10,\n",
      " 'HAS': 10,\n",
      " 'Hallo': 10,\n",
      " 'Have': 20,\n",
      " 'Hello': 120,\n",
      " 'Helo': 10,\n",
      " 'Here': 10,\n",
      " 'Highs': 30,\n",
      " 'I': 430,\n",
      " 'IS': 20,\n",
      " 'If': 10,\n",
      " 'Is': 10,\n",
      " 'It': 50,\n",
      " 'JUNE': 10,\n",
      " 'June': 210,\n",
      " 'Juni': 10,\n",
      " 'Junio': 10,\n",
      " 'KANTA': 10,\n",
      " 'Kh√¥ng': 10,\n",
      " 'LOVE': 10,\n",
      " 'Listening': 10,\n",
      " 'Love': 10,\n",
      " 'Loves': 10,\n",
      " 'MONTH': 10,\n",
      " 'Mastercard': 10,\n",
      " 'Maybe': 10,\n",
      " 'Me': 30,\n",
      " 'My': 10,\n",
      " 'M√∫sica': 10,\n",
      " 'New': 10,\n",
      " 'OF': 10,\n",
      " 'Off': 10,\n",
      " 'One': 10,\n",
      " 'Ooh': 30,\n",
      " 'Oui': 10,\n",
      " 'Please': 10,\n",
      " 'Pool': 10,\n",
      " 'Present': 10,\n",
      " 'Que': 10,\n",
      " 'R.I.P': 10,\n",
      " 'RD': 10,\n",
      " 'Rd': 10,\n",
      " 'SARII≈ûIK': 10,\n",
      " 'She': 10,\n",
      " 'Skitbot': 10,\n",
      " 'So': 10,\n",
      " 'THIS': 30,\n",
      " 'Tahir': 10,\n",
      " 'Tajikistan': 10,\n",
      " 'Temon': 10,\n",
      " 'Thank': 10,\n",
      " 'That': 10,\n",
      " 'The': 10,\n",
      " 'There': 10,\n",
      " 'They': 10,\n",
      " 'This': 10,\n",
      " 'To': 80,\n",
      " 'Turkey': 10,\n",
      " 'T√ºrkiye': 10,\n",
      " 'Ultra': 10,\n",
      " 'VEIWS': 10,\n",
      " 'VID': 10,\n",
      " 'VmESteüòâ': 10,\n",
      " 'WHAT': 10,\n",
      " 'WOMEN': 10,\n",
      " 'What': 10,\n",
      " 'When': 10,\n",
      " 'Who': 20,\n",
      " 'Why': 10,\n",
      " 'YOU': 10,\n",
      " 'Yess': 10,\n",
      " 'Zambia': 10,\n",
      " 'a': 110,\n",
      " 'about': 20,\n",
      " 'actor': 10,\n",
      " 'adele': 10,\n",
      " 'after': 10,\n",
      " 'ago': 10,\n",
      " 'ai': 10,\n",
      " 'all': 10,\n",
      " 'alot': 10,\n",
      " 'always': 20,\n",
      " 'am': 40,\n",
      " 'amazing': 10,\n",
      " 'an': 10,\n",
      " 'and': 70,\n",
      " 'annoying': 10,\n",
      " 'another': 10,\n",
      " 'any': 10,\n",
      " 'anymore': 60,\n",
      " 'anyone': 40,\n",
      " 'apart': 30,\n",
      " 'are': 30,\n",
      " 'at': 50,\n",
      " 'a√≠': 10,\n",
      " 'be': 40,\n",
      " 'beautiful': 10,\n",
      " 'beautiful-voiced': 10,\n",
      " 'beautifulüòÉ': 10,\n",
      " 'because': 10,\n",
      " 'before': 10,\n",
      " 'best': 20,\n",
      " 'better': 20,\n",
      " 'between': 10,\n",
      " 'blaster': 10,\n",
      " 'born': 10,\n",
      " 'both': 10,\n",
      " 'breaking': 30,\n",
      " 'burn': 10,\n",
      " 'burns': 20,\n",
      " 'but': 20,\n",
      " 'by': 10,\n",
      " 'ca': 30,\n",
      " 'call': 30,\n",
      " 'called': 30,\n",
      " 'calling': 10,\n",
      " 'can': 40,\n",
      " 'ceet': 10,\n",
      " 'cg': 10,\n",
      " 'clasico': 10,\n",
      " 'clasicos': 10,\n",
      " 'clearly': 30,\n",
      " 'cross': 10,\n",
      " 'da': 10,\n",
      " 'de': 10,\n",
      " 'depuis': 10,\n",
      " 'deste': 10,\n",
      " 'did': 10,\n",
      " 'difference': 10,\n",
      " 'digan': 10,\n",
      " 'disappear': 10,\n",
      " 'do': 30,\n",
      " 'does': 30,\n",
      " 'dollars': 10,\n",
      " 'done': 40,\n",
      " 'dreaming': 10,\n",
      " 'drunk': 10,\n",
      " 'e': 10,\n",
      " 'easy': 10,\n",
      " 'es': 10,\n",
      " 'esto': 10,\n",
      " 'euros': 10,\n",
      " 'ever': 30,\n",
      " 'everything': 40,\n",
      " 'exist√™ncia': 10,\n",
      " 'eyes': 10,\n",
      " 'feet': 10,\n",
      " 'fell': 10,\n",
      " 'felt': 10,\n",
      " 'first': 10,\n",
      " 'for': 130,\n",
      " 'forgotten': 10,\n",
      " 'free': 10,\n",
      " 'from': 110,\n",
      " 'gets': 10,\n",
      " 'go': 10,\n",
      " 'got': 20,\n",
      " 'happened': 10,\n",
      " 'happens': 10,\n",
      " 'happy': 10,\n",
      " 'hard': 20,\n",
      " 'having': 20,\n",
      " 'heal': 10,\n",
      " 'healing': 10,\n",
      " 'healthy': 10,\n",
      " 'hear': 10,\n",
      " 'heard': 10,\n",
      " 'heart': 50,\n",
      " 'hearts': 10,\n",
      " 'hello': 10,\n",
      " 'help': 10,\n",
      " 'her': 20,\n",
      " 'here': 60,\n",
      " 'hermoso': 10,\n",
      " 'highs': 90,\n",
      " 'history': 10,\n",
      " 'hits': 10,\n",
      " 'hoa': 10,\n",
      " 'home': 30,\n",
      " 'hope': 20,\n",
      " 'how': 30,\n",
      " 'h√†o': 10,\n",
      " 'i': 20,\n",
      " 'if': 40,\n",
      " 'im': 10,\n",
      " 'imax': 10,\n",
      " 'in': 210,\n",
      " 'incredible': 10,\n",
      " 'india': 10,\n",
      " 'is': 60,\n",
      " 'it': 120,\n",
      " 'jun2_': 10,\n",
      " 'june': 50,\n",
      " 'junio': 10,\n",
      " 'just': 10,\n",
      " 'know': 10,\n",
      " \"l'Afrique\": 10,\n",
      " 'la': 20,\n",
      " 'late': 10,\n",
      " 'least': 30,\n",
      " 'leave': 10,\n",
      " 'legend': 10,\n",
      " 'li': 10,\n",
      " 'life': 10,\n",
      " 'like': 10,\n",
      " 'likes': 10,\n",
      " 'linda‚ù§‚ù§‚ù§': 10,\n",
      " 'listen': 10,\n",
      " 'listening': 20,\n",
      " 'look': 40,\n",
      " 'los': 10,\n",
      " 'love': 30,\n",
      " 'loves': 10,\n",
      " 'lows': 160,\n",
      " 'mais': 10,\n",
      " 'make': 20,\n",
      " 'matter': 30,\n",
      " 'max': 10,\n",
      " 'may': 10,\n",
      " 'me': 120,\n",
      " 'meet': 10,\n",
      " 'mejor': 10,\n",
      " 'melhores': 10,\n",
      " 'memories': 10,\n",
      " 'mi': 10,\n",
      " 'miles': 10,\n",
      " 'million': 10,\n",
      " 'miss': 10,\n",
      " 'mom': 10,\n",
      " 'mong': 10,\n",
      " 'most': 10,\n",
      " 'much': 30,\n",
      " 'mueren': 10,\n",
      " 'music': 10,\n",
      " 'must': 30,\n",
      " 'mv': 10,\n",
      " 'my': 40,\n",
      " 'myself': 10,\n",
      " 'm·ªôt': 10,\n",
      " \"n't\": 100,\n",
      " 'nd': 10,\n",
      " 'ne': 10,\n",
      " 'need': 10,\n",
      " 'never': 30,\n",
      " 'nice': 10,\n",
      " 'night': 10,\n",
      " 'no': 10,\n",
      " 'not': 30,\n",
      " 'nothing': 10,\n",
      " 'nunca': 10,\n",
      " 'of': 70,\n",
      " 'one': 30,\n",
      " 'only': 20,\n",
      " 'or': 10,\n",
      " 'os': 10,\n",
      " 'other': 60,\n",
      " 'our': 20,\n",
      " 'out': 20,\n",
      " 'outside': 50,\n",
      " 'over': 10,\n",
      " 'payed': 10,\n",
      " 'pena': 10,\n",
      " 'pequena': 10,\n",
      " 'person': 10,\n",
      " 'planeta': 10,\n",
      " 'plz': 10,\n",
      " 'poem': 10,\n",
      " 'pro': 10,\n",
      " 'pust': 10,\n",
      " 'reading': 10,\n",
      " 'road': 10,\n",
      " 'running': 10,\n",
      " 's': 10,\n",
      " 'same': 10,\n",
      " 'say': 50,\n",
      " 'saying': 10,\n",
      " 'secret': 10,\n",
      " 'seem': 30,\n",
      " 'sending': 10,\n",
      " 'she': 10,\n",
      " 'side': 60,\n",
      " 'sing': 10,\n",
      " 'singing': 10,\n",
      " 'smartass': 10,\n",
      " 'so': 20,\n",
      " 'someone': 10,\n",
      " 'something': 10,\n",
      " 'song': 40,\n",
      " 'songbird': 10,\n",
      " 'songs': 10,\n",
      " 'songüíñ': 10,\n",
      " 'sorry': 70,\n",
      " 'stand': 10,\n",
      " 'still': 10,\n",
      " 'successful': 10,\n",
      " 'such': 10,\n",
      " 'supposed': 10,\n",
      " 'sure': 10,\n",
      " 's√≥': 10,\n",
      " 'talented': 10,\n",
      " 'talentos': 10,\n",
      " 'talk': 10,\n",
      " 'tear': 30,\n",
      " 'tell': 60,\n",
      " 'thank': 10,\n",
      " 'that': 150,\n",
      " 'the': 150,\n",
      " 'there': 10,\n",
      " 'these': 10,\n",
      " 'think': 10,\n",
      " 'this': 80,\n",
      " 'those': 20,\n",
      " 'thousand': 50,\n",
      " 'ti': 10,\n",
      " 'time': 30,\n",
      " 'timeless': 10,\n",
      " 'times': 50,\n",
      " 'to': 110,\n",
      " 'too': 10,\n",
      " 'town': 10,\n",
      " 'tried': 50,\n",
      " 'typical': 10,\n",
      " 'u': 10,\n",
      " 'un': 10,\n",
      " 'us': 20,\n",
      " 'used': 10,\n",
      " 'vaoüòÆcedo': 10,\n",
      " 'vie': 10,\n",
      " 'voice': 30,\n",
      " 'was': 20,\n",
      " 'watch': 10,\n",
      " 'watching': 20,\n",
      " 'way': 10,\n",
      " 'we': 20,\n",
      " 'well': 10,\n",
      " 'were': 20,\n",
      " 'what': 20,\n",
      " 'when': 30,\n",
      " 'where': 10,\n",
      " 'who': 20,\n",
      " 'wind': 10,\n",
      " 'with': 10,\n",
      " 'woman': 20,\n",
      " 'wonderful': 10,\n",
      " 'wondering': 10,\n",
      " 'world': 10,\n",
      " 'writing': 10,\n",
      " 'y': 10,\n",
      " 'ya': 10,\n",
      " 'years': 20,\n",
      " 'you': 310,\n",
      " 'younger': 10,\n",
      " 'your': 60,\n",
      " 'youüòòüòòüòòüòòüòò‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è‚ù§Ô∏è': 10,\n",
      " 'ƒë·ªùi': 10,\n",
      " '–ê–î–ï–õ–¨': 10,\n",
      " '–î–∞': 10,\n",
      " '–ö—Ç–æ': 10,\n",
      " '–ú—É—Ä–∞—à–∫–∏': 10,\n",
      " '–¢–´': 10,\n",
      " '–¢—ã': 10,\n",
      " '–ß–£–î–û': 10,\n",
      " '–¥–æ—Ä–æ–≥–∞—è': 10,\n",
      " '–µ—Å—Ç—å': 10,\n",
      " '–∏': 10,\n",
      " '–∏–º–∞–Ω–¥–∂–∏–Ω–∞—Ä–∏—É–º–∞': 10,\n",
      " '–∫–æ–∂–µ': 10,\n",
      " '–∫—Ä–∞—Å–∏–≤–∞—è': 10,\n",
      " '–ª—É—á—à–∞—è': 10,\n",
      " '–æ—Ç': 10,\n",
      " '–æ—á–µ–Ω—å': 10,\n",
      " '–ø–µ—Å–Ω–∏': 10,\n",
      " '–ø–æ': 10,\n",
      " '—Å': 10,\n",
      " '—Ç–∞–ª–∞–Ω—Ç–ª–∏–≤–∞—è': 10,\n",
      " '‡∏Æ‡∏±‡∏•‡πÇ‡∏´‡∏•‚ù§': 10,\n",
      " '‚Äô': 10,\n",
      " '‚Äú': 10,\n",
      " '‚Äù': 10,\n",
      " '‚ô•Ô∏è': 10,\n",
      " '‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è‚ô•Ô∏è': 10,\n",
      " '‚ú®2024‚ú®': 10,\n",
      " '‚ú®‚òÄÔ∏è‚ú®': 10,\n",
      " '‚ù£Ô∏è': 10,\n",
      " '‚ù§': 80,\n",
      " '‚ù§canta': 10,\n",
      " '‚ù§desde': 10,\n",
      " '‚ù§‚ù§': 10,\n",
      " '‚ù§‚ù§‚ù§‚ù§': 10,\n",
      " '‚ù§‚ù§‚ù§üî•üé§üî•‚ù§‚ù§‚ù§': 10,\n",
      " '‚ù§üòä': 10,\n",
      " 'üáßüá∑': 10,\n",
      " 'üá®üáÆ': 10,\n",
      " 'üáπüáØ‚ù§': 10,\n",
      " 'üéâ‚ù§': 10,\n",
      " 'üéâ‚ù§üéâ': 10,\n",
      " 'üéâüéâ‚ù§': 10,\n",
      " 'üé∂hello': 10,\n",
      " 'üé∂ü§£ü§£': 10,\n",
      " 'üíñüíñü™Ñ\\U0001faa9üíÉüèº\\U0001faa9üëèüèºüëèüèº\\U0001fabd': 10,\n",
      " 'üî•üî•üé§üî•üî•‚ù§‚ù§‚ù§‚ù§': 10,\n",
      " 'üî•üî•üî•': 10,\n",
      " 'üòÅüëçüèºShe': 10,\n",
      " 'üòÑ': 10,\n",
      " 'üòÖon': 10,\n",
      " 'üòî': 10,\n",
      " 'üò≠üò≠üò≠': 10,\n",
      " 'ü•∞': 20}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# ÊåáÂÆöExcel Ê™îÊ°àË∑ØÂæë\n",
    "excel_file_path = 'C:/Users/USER/HelloAdele.xlsx'\n",
    "\n",
    "# ËÆÄÂèñ Excel Ê™îÊ°à‰∏≠ÁöÑÊñáÊú¨ÔºåÂÅáË®≠ÊñáÊú¨ÊâÄÂú®ÁöÑÊ¨Ñ‰ΩçÁÇ∫ 'ColumnName'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "text_column = df['Text'].tolist()\n",
    "\n",
    "# ‰ΩøÁî® nltk Êñ∑Ë©û\n",
    "tokenized_text = [word_tokenize(sentence) for sentence in text_column]\n",
    "\n",
    "# ÊâìÂç∞Êñ∑Ë©ûÂæåÁöÑÁµêÊûú\n",
    "# pprint(tokenized_text, width=79, compact=True)\n",
    "\n",
    "# Â∞áÊâÄÊúâÂè•Â≠êÁöÑÊñ∑Ë©ûÂêà‰ΩµÊàê‰∏ÄÂÄãÂàóË°®\n",
    "words = [word for sentence_words in tokenized_text for word in sentence_words]\n",
    "\n",
    "# ‰ΩøÁî® FreqDist Ë®àÁÆóË©ûÈ†ª\n",
    "fd = nltk.FreqDist(words)\n",
    "\n",
    "# ÊâìÂç∞Ë©ûÈ†ªÁµ±Ë®à\n",
    "pprint(fd, width=79, compact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "2d8ba70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<googleapiclient.discovery.Resource object at 0x0000025F6AAFCD60>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import openpyxl\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "DEVELOPER_KEY = 'AIzaSyAROQxkWBSPUUVFXk70tHj_P52_VK1ediM'\n",
    "youtubeapi = build('youtube', 'v3', developerKey=DEVELOPER_KEY)\n",
    "\n",
    "print(youtubeapi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "4e65ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year                    song  pop             artist   dnce   enry  key  \\\n",
      "0     2000  Oops!...I Did It Again   81     Britney Spears  0.751  0.834    1   \n",
      "1     2000    All The Small Things   83          blink-182  0.434  0.897    0   \n",
      "2     2000                 Breathe   66         Faith Hill  0.529  0.496    7   \n",
      "3     2000            It's My Life   81           Bon Jovi  0.551  0.913    0   \n",
      "4     2000             Bye Bye Bye   75              NSYNC  0.610  0.926    8   \n",
      "...    ...                     ...  ...                ...    ...    ...  ...   \n",
      "2499  2023                    exes   56         Tate McRae  0.838  0.569    5   \n",
      "2500  2023                   QLONA   79            KAROL G  0.842  0.756    7   \n",
      "2501  2023              LOVE AGAIN   69      The Kid LAROI  0.662  0.398   11   \n",
      "2502  2023                 Feather   91  Sabrina Carpenter  0.787  0.686    6   \n",
      "2503  2023               Attention   67           Doja Cat  0.759  0.567    2   \n",
      "\n",
      "         db  mode    spch  ...      bpm     dur  time_signature  rank    neg  \\\n",
      "0    -5.444     0  0.0437  ...   95.053  211160               4  55.0  0.090   \n",
      "1    -4.918     1  0.0488  ...  148.726  167067               4  40.0  0.112   \n",
      "2    -9.007     1  0.0290  ...  136.859  250547               4   1.0  0.067   \n",
      "3    -4.063     0  0.0466  ...  119.992  224493               4   NaN  0.015   \n",
      "4    -4.843     0  0.0479  ...  172.638  200400               4  21.0  0.094   \n",
      "...     ...   ...     ...  ...      ...     ...             ...   ...    ...   \n",
      "2499 -6.324     0  0.0621  ...  136.965  159400               4   NaN    NaN   \n",
      "2500 -7.409     0  0.3380  ...  169.925  172798               4   NaN    NaN   \n",
      "2501 -6.691     0  0.0275  ...  107.001  145850               4   NaN    NaN   \n",
      "2502 -4.370     0  0.0339  ...  123.510  185553               4   NaN    NaN   \n",
      "2503 -7.911     1  0.2480  ...   87.981  277044               4   NaN    NaN   \n",
      "\n",
      "        neu    pos      Likes         Views  Comments  \n",
      "0     0.729  0.181  2872003.0  4.532297e+08  109947.0  \n",
      "1     0.752  0.136  1794874.0  3.746740e+08   64050.0  \n",
      "2     0.717  0.215   224609.0  3.560741e+07    8368.0  \n",
      "3     0.923  0.062  7449619.0  1.388075e+09  200218.0  \n",
      "4     0.776  0.130  1613385.0  3.602241e+08   75454.0  \n",
      "...     ...    ...        ...           ...       ...  \n",
      "2499    NaN    NaN   382659.0  3.223234e+07   10410.0  \n",
      "2500    NaN    NaN   930990.0  1.934198e+08   22313.0  \n",
      "2501    NaN    NaN   357848.0  2.203794e+07   14208.0  \n",
      "2502    NaN    NaN  1008146.0  5.895336e+07   24024.0  \n",
      "2503    NaN    NaN   808621.0  2.911061e+07   34423.0  \n",
      "\n",
      "[2504 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "songlist = pd.read_csv('result2.csv',encoding='cp1252')\n",
    "resultsonglist = songlist.copy()\n",
    "print(songlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0565ece8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "words = [w for w in words if w.lower() not in stopwords]\n",
    "\n",
    "def nltkuse(pathfile, resultcount):\n",
    "    # ÊåáÂÆöExcel Ê™îÊ°àË∑ØÂæë\n",
    "\n",
    "    # ËÆÄÂèñ Excel Ê™îÊ°à‰∏≠ÁöÑÊñáÊú¨ÔºåÂÅáË®≠ÊñáÊú¨ÊâÄÂú®ÁöÑÊ¨Ñ‰ΩçÁÇ∫ 'ColumnName'\n",
    "    df = pd.read_excel(pathfile)\n",
    "    text_column = df['Text'].tolist()\n",
    "\n",
    "    tokenized_text = [word_tokenize(sentence) for sentence in text_column]\n",
    "\n",
    "    words = [word for sentence_words in tokenized_text for word in sentence_words]\n",
    "\n",
    "    sia = SIA()\n",
    "    text=' '.join(words)\n",
    "    scores = sia.polarity_scores(text)\n",
    "\n",
    "    resultsonglist['neg'][resultcount] = scores['neg']\n",
    "    resultsonglist['pos'][resultcount] = scores['pos']\n",
    "    resultsonglist['neu'][resultcount] = scores['neu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e47b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Áà¨Ëü≤Â§±ÊïóÁöÑË©±Ë®òÈåÑÁî®\n",
    "def noin(songname,artist):\n",
    "    if '?' in songname or '?' in artist:\n",
    "        songname = songname.replace('?','')\n",
    "    elif ':' in songname or '?' in artist:\n",
    "        songname = songname.replace(':','')\n",
    "    elif '/' in songname or '?' in artist:\n",
    "        songname = songname.replace('/','')\n",
    "    elif '|' in songname or '?' in artist:\n",
    "        songname = songname.replace('|','')\n",
    "    elif '*' in songname or '?' in artist:\n",
    "        songname = songname.replace('*','')\n",
    "    elif '\"' in songname or '?' in artist:\n",
    "        songname = songname.replace('\"','')\n",
    "    elif '>' in songname or '?' in artist:\n",
    "        songname = songname.replace('>','')\n",
    "    elif '<' in songname or '?' in artist:\n",
    "        songname = songname.replace('<','')\n",
    "    elif ',' in songname or '?' in artist:\n",
    "        songname = songname.replace(',','')\n",
    "    excel_file_path = './temptxt/' + artist + '-' + songname +'.txt'\n",
    "    f = open('./temptxt/all.txt','w',encoding='utf-8')\n",
    "    data = f.read()\n",
    "    writeuse = data + excel_file_path\n",
    "    f.writeline(writeuse)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "ad9ca712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtubecomment2(videoid,max_results,songname,artist):\n",
    "    #max_results = 300\n",
    "    total_comment = []\n",
    "    commentcount = 0\n",
    "    page_token = None\n",
    "    noinbool = False #Áî®‰æÜÂà§Êñ∑ÊúâÊ≤íÊúâÂ∞èÊñº1000\n",
    "    while len(total_comment) < max_results:\n",
    "        request = youtubeapi.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=videoid,\n",
    "            textFormat='plainText',\n",
    "            maxResults=100,\n",
    "            pageToken=page_token\n",
    "        )\n",
    "\n",
    "        response = request.execute()\n",
    "        #Âà§Êñ∑ÊúâÊ≤íÊúâÂ∞èÊñº1000 ÊúâÁöÑË©±noinboolËÆäTrue ‰∏ãÈù¢ÂõûÂÇ≥Á©∫ÂÄº\n",
    "        #Â¶ÇÊûúÂ∞èÊñº1000‰πüË¶ÅÁöÑË©± Áúã‰∏ãÈù¢***Ë®ªËß£\n",
    "        if commentcount ==  len(response.get('items')):\n",
    "            noin(songname,artist)\n",
    "            noinbool = True\n",
    "            break\n",
    "        commentcount += 1\n",
    "        for comment in response.get('items', []):\n",
    "            author = comment['snippet']['topLevelComment']['snippet']['authorDisplayName']\n",
    "            text = comment['snippet']['topLevelComment']['snippet']['textDisplay']\n",
    "            total_comment.append({'author': author, 'text': text})\n",
    "    # ***Ë®ªËß£ Â¶ÇÊûúÊï∏ÈáèÂ∞èÊñº1000‰πüË¶ÅÁî®ÁöÑË©± Áõ¥Êé•return total_commentÂ∞±Â•Ω‰∫Ü\n",
    "    if noinbool == False:\n",
    "        return total_comment\n",
    "    else:\n",
    "        total_comment = []\n",
    "        return total_comment\n",
    "    \n",
    "\n",
    "\n",
    "def getcommon(videoIds,songname,artist):\n",
    "    max_results1=1000\n",
    "    # \\ / ? : * \" > < | windowsÂ≠òÊ™îÈôêÂà∂\n",
    "    if '?' in songname or '?' in artist:\n",
    "        songname = songname.replace('?','')\n",
    "    elif ':' in songname or '?' in artist:\n",
    "        songname = songname.replace(':','')\n",
    "    elif '/' in songname or '?' in artist:\n",
    "        songname = songname.replace('/','')\n",
    "    elif '|' in songname or '?' in artist:\n",
    "        songname = songname.replace('|','')\n",
    "    elif '*' in songname or '?' in artist:\n",
    "        songname = songname.replace('*','')\n",
    "    elif '\"' in songname or '?' in artist:\n",
    "        songname = songname.replace('\"','')\n",
    "    elif '>' in songname or '?' in artist:\n",
    "        songname = songname.replace('>','')\n",
    "    elif '<' in songname or '?' in artist:\n",
    "        songname = songname.replace('<','')\n",
    "    elif ',' in songname or '?' in artist:\n",
    "        songname = songname.replace(',','')\n",
    "\n",
    "    excel_file_path = './tempcsv1/' + artist + '-' + songname +'.xlsx'\n",
    "    comment2=youtubecomment2(videoIds,max_results1,songname,artist)\n",
    "    #Â¶ÇÊûúÂ∞èÊñº1000‰∏çÂ≠òÔºåË¶ÅÂ≠òÂ∞èÊñº1000ÁöÑÊääifÊ¢ù‰ª∂ÊãøÊéâ\n",
    "    if len(comment2) >= 1000:\n",
    "        wb = openpyxl.Workbook()\n",
    "        ws = wb.active\n",
    "\n",
    "\n",
    "        ws.append(['Author', 'Text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for comment in comment2:\n",
    "            ws.append([comment['author'], comment['text']])\n",
    "\n",
    "        wb.save(excel_file_path)\n",
    "        return excel_file_path\n",
    "    # print(f'Comments data has been saved to {excel_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "6ea56b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|‚ñè         | 45/2504 [01:51<1:41:21,  2.47s/it]\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "not readable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\2062873313.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m                         \u001b[0mtempsongid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtruesongurl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                         \u001b[0mexcel_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetcommon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempsongid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msonglist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'song'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msonglist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'artist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m                         \u001b[0mnltkuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexcel_file_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\351074387.py\u001b[0m in \u001b[0;36mgetcommon\u001b[1;34m(videoIds, songname, artist)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mexcel_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./tempcsv1/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msongname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.xlsx'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mcomment2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myoutubecomment2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideoIds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_results1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msongname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m     \u001b[1;31m#Â¶ÇÊûúÂ∞èÊñº1000‰∏çÂ≠òÔºåË¶ÅÂ≠òÂ∞èÊñº1000ÁöÑÊääifÊ¢ù‰ª∂ÊãøÊéâ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\351074387.py\u001b[0m in \u001b[0;36myoutubecomment2\u001b[1;34m(videoid, max_results, songname, artist)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcommentcount\u001b[0m \u001b[1;33m==\u001b[0m  \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'items'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0mnoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msongname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0martist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mnoinbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\1044216745.py\u001b[0m in \u001b[0;36mnoin\u001b[1;34m(songname, artist)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./temptxt/all.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mwriteuse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexcel_file_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not readable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\2062873313.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[0mnoselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m                 \u001b[0mnoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msonglist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'song'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msonglist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'artist'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m                 \u001b[1;31m#tempÁúãÂ†±ÈåØÁöÑ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m                 \u001b[1;31m# temp = str(e)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5088\\1044216745.py\u001b[0m in \u001b[0;36mnoin\u001b[1;34m(songname, artist)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mexcel_file_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./temptxt/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0martist\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'-'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msongname\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;34m'.txt'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./temptxt/all.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mwriteuse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mexcel_file_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriteline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwriteuse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnsupportedOperation\u001b[0m: not readable"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "tempcount = 0\n",
    "noselect = []\n",
    "with tqdm (total=len(songlist)) as pbar:\n",
    "    for i in range(0,len(songlist)):\n",
    "        # tempcount += 1\n",
    "        # if tempcount >10:\n",
    "        #     break\n",
    "        #Âà§Êñ∑3ÂÄãÈÉΩÊ≤íÊúâ‰ª£Ë°®Ê≤íÂÅö Âè™ÂÅöÊ≤íÂÅöÈÅéÁöÑ\n",
    "        if math.isnan(songlist['pos'][i]) == True and math.isnan(songlist['neu'][i]) == True and math.isnan(songlist['neg'][i]) == True:\n",
    "            #youtubeÁ∂≤ÂùÄ‰∏≠ÁöÑ?‰πüÊòØ‰∏ÄÂÄãÂà§Êñ∑Ê¢ù‰ª∂ Â∞áÂÖ∂ÂéªÈô§\n",
    "            if '?' in songlist['song'][i]:\n",
    "                songlist['song'][i] = songlist['song'][i].replace('?','')\n",
    "            if '?' in songlist['artist'][i]:\n",
    "                songlist['artist'][i] = songlist['artist'][i].replace('?','')\n",
    "            #ÁµÑÊàêÊ≠åÊõ≤ÊêúÂ∞ãurl\n",
    "            songnametemp = '+'.join(songlist['song'][i].split(' '))\n",
    "            songurl = 'https://www.youtube.com/results?search_query=' + songlist['artist'][i]+'+' + songnametemp\n",
    "            # print(songurl)\n",
    "\n",
    "            #Áà¨Ëü≤\n",
    "            #Ë®≠ÂÆöChrome driverÂü∑Ë°åÊ™îË∑ØÂæë(ÂàáË®ò!chrome driverË¶ÅË∑üÊâÄ‰ΩøÁî®ÁöÑchromeÁÄèË¶ΩÂô®ÁâàÊú¨Áõ∏Á¨¶)\n",
    "            options=Options()#ÈÄèÈÅéOptionsÁâ©‰ª∂ÈÄ≤Ë°åchromeÁöÑË®≠ÂÆö\n",
    "            options.chrome_executable_path = \"C:\\chrome-win64\\chrome\"\n",
    "            driver = webdriver.Chrome(options=option)\n",
    "            # driver = webdriver.Chrome()\n",
    "            driver.get(songurl)\n",
    "            #Áç≤ÂèñÊ≠åÊõ≤\n",
    "            video_elements = driver.find_elements(By.XPATH, '//yt-formatted-string[@class=\"style-scope ytd-video-renderer\"]')\n",
    "            try:\n",
    "                for video_element in video_elements:\n",
    "                    aria_label = video_element.get_attribute('aria-label')\n",
    "                    #Ê≠£Á¢∫ÊêúÂ∞ãÊ≠åÊõ≤ÁöÑË©±ÂÆòÊñπÂΩ±ÁâáÊúÉÂú®ÊúÄ‰∏äÈù¢ Áõ¥Êé•Â∞çÊúâËßÄÁúãÊ¨°Êï∏ÁöÑÈÄ≤Ë°åÈªûÊìä\n",
    "                    if aria_label and 'ËßÄÁúãÊ¨°Êï∏' in aria_label:\n",
    "                        video_element.click()\n",
    "                        #Áç≤ÂèñÊ≠åÊõ≤È†ÅÈù¢url\n",
    "                        truesongurl = driver.current_url\n",
    "                        #ÂèñÂæóID\n",
    "                        tempsongid = truesongurl.split('=')\n",
    "                        excel_file_path = getcommon(tempsongid[1],songlist['song'][i],songlist['artist'][i])\n",
    "                        nltkuse(excel_file_path,i)\n",
    "                        pbar.update(1)\n",
    "                        driver.quit()\n",
    "                        break\n",
    "            except Exception as e:\n",
    "                noselect.append(i)\n",
    "                noin(songlist['song'][i],songlist['artist'][i])\n",
    "                #tempÁúãÂ†±ÈåØÁöÑ\n",
    "                # temp = str(e)\n",
    "                # temp1 = temp.split(\"'reason': '\")\n",
    "                # temp2 = temp1[1].split(\"',\")\n",
    "                # print(str(i) + temp2[0])\n",
    "                pbar.update(1)\n",
    "                driver.quit()\n",
    "        else:\n",
    "            pbar.update(1)\n",
    "    print(len(noselect))\n",
    "    resultsonglist.to_csv('result.csv',index=False)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "dcb8daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsonglist.to_csv('result2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0b15f17b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.078, 'neu': 0.783, 'pos': 0.139, 'compound': 1.0}\n"
     ]
    }
   ],
   "source": [
    "text=' '.join(words)\n",
    "scores = sia.polarity_scores(text)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a93121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  , the   . \n",
      "450 380 320 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd.most_common(3)\n",
    "\n",
    "fd.tabulate(3)\n",
    "\n",
    "lower_fd = nltk.FreqDist([w.lower() for w in fd])\n",
    "\n",
    "fd[\"love\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4fe8ae5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 1079 matches:\n",
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n",
      "beyond any shadow of a doubt , that America will continue the fight for freedom\n",
      " to make complete victory certain , America will never become a party to any pl\n",
      "nly in law and in justice . Here in America , we have labored long and hard to \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(nltk.corpus.state_union.words())\n",
    "text.concordance(\"america\", lines=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ca29a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " would want us to do . That is what America will do . So much blood has already\n",
      "ay , the entire world is looking to America for enlightened leadership to peace\n"
     ]
    }
   ],
   "source": [
    "concordance_list = text.concordance_list(\"america\", lines=2)\n",
    "for entry in concordance_list:\n",
    "    print(entry.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03c52d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  , the   . \n",
      "450 380 320 \n"
     ]
    }
   ],
   "source": [
    "text = nltk.Text(words)#Âà©Áî®vocabÈáçÊñ∞Ë®àÁÆóË©ûÈ†ª\n",
    "fd = text.vocab()\n",
    "fd.tabulate(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fc9de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('the', 'United', 'States') ('the', 'American', 'people') \n",
      "                          294                           185 \n"
     ]
    }
   ],
   "source": [
    "words = [w for w in nltk.corpus.state_union.words() if w.isalpha()]\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_words(words)\n",
    "finder.ngram_fd.most_common(2)\n",
    "finder.ngram_fd.tabulate(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2903c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import pandas as pd\n",
    "\n",
    "# dataset = pd.read_excel(\"D:\\\\youtube_comments5.xlsx\")\n",
    "# for i in range(len(dataset)):\n",
    "#     list1=dataset['Text'][i]\n",
    "#     words: list[str] = nltk.word_tokenize(list1)\n",
    "#     fd = nltk.FreqDist(words)\n",
    "#     pprint(fd, width=79, compact=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9ff0fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores(\"Wow, NLTK is really powerful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13facd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5022c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> False @leiwaleyn @sophiasam013 miss you too laine :(\n",
      "> False RT @Martin_L_1874: IF THE LIEBOUR PARTY DONT WORK WITH SNP WE DEFO NEED MORE SNP MPs TO FIGHT TORIES, RED EDD IS A DISGRACE AS NO RESPECT T‚Ä¶\n",
      "> False RT @patcondell: Farage spells it out. We're heading for disaster. #EU stubbornness will allow ISIS to flood Europe with terrorists. https:/‚Ä¶\n",
      "> True RT @_Normina_: Many fabulous #SNPbecause tweets; &amp; it's trending at number 1 in Edinburgh!! Vote #SNP... In Livingston vote @HannahB4LiviMP‚Ä¶\n",
      "> False @korsikoff Strangely enough, the number of points you'll get all year too :))))\n",
      "> False @johnmcternan @lenathehyena @UKLabour @Ed_Miliband Try going to Queen to form a Gov with fewer MPs than Tories and with no deal in place.\n",
      "> True Photo: randy9bis: Beautiful physique, shaved, uncut, and a tattoo: sexy boi !¬† :-) http//t.co/XcWxo5jVVY\n",
      "> False RT @UKIP: '@Nigel_Farage on HSBC and the current regulatory regime that makes London too expensive a place to do business http//t.co/95SxB‚Ä¶\n",
      "> False RT @MirrorPolitics: Ed Miliband tells #BBCQT he'd rather lose No10 than do SNP deal http//t.co/P0IAJX1glu http//t.co/jRSXhoj3JK\n",
      "> True @screwlabour ukip is following more of a gumbel trend but the rest are definitely normal. Use your standard deviation analysis!!!\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "def is_positive(tweet: str) -> bool:\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", is_positive(tweet), tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e7e6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b157ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def is_positive(review_id: str) -> bool:\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "025a0349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.05% correct\n"
     ]
    }
   ],
   "source": [
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive(review_id):\n",
    "         if review_id in positive_review_ids:\n",
    "                correct += 1\n",
    "    else:\n",
    "         if review_id in negative_review_ids:\n",
    "                correct += 1\n",
    "                \n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36831c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ed14a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cc5b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "863f8f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6830539",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff29856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 2                 pos : neg    =      7.3 : 1.0\n",
      "               wordcount = 3                 pos : neg    =      6.3 : 1.0\n",
      "               wordcount = 4                 pos : neg    =      1.8 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.7 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.5 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6533333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "122b220f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "edd9f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import (\n",
    "    BernoulliNB,\n",
    "    ComplementNB,\n",
    "    MultinomialNB,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f11a9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"BernoulliNB\": BernoulliNB(),\n",
    "    \"ComplementNB\": ComplementNB(),\n",
    "    \"MultinomialNB\": MultinomialNB(),\n",
    "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"LogisticRegression\": LogisticRegression(),\n",
    "    \"MLPClassifier\": MLPClassifier(max_iter=1000),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57d0c2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60743c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.60% - BernoulliNB\n",
      "66.33% - ComplementNB\n",
      "65.20% - MultinomialNB\n",
      "68.47% - KNeighborsClassifier\n",
      "63.73% - DecisionTreeClassifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.47% - RandomForestClassifier\n",
      "71.00% - LogisticRegression\n",
      "72.53% - MLPClassifier\n",
      "70.53% - AdaBoostClassifier\n"
     ]
    }
   ],
   "source": [
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "for name, sklearn_classifier in classifiers.items():\n",
    "    classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
    "    classifier.train(features[:train_count])\n",
    "    accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
    "    print(F\"{accuracy:.2%} - {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
